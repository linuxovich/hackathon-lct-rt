version: "3.9"

services:
  rabbit:
    image: rabbitmq:3.13-management
    container_name: rabbit
    restart: unless-stopped
    ports:
      - "5672:5672"    # AMQP
      - "15672:15672"  # Web UI
    environment:
      RABBITMQ_DEFAULT_USER: appuser
      RABBITMQ_DEFAULT_PASS: supersecret
      RABBITMQ_DEFAULT_VHOST: /
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 5s
      timeout: 5s
      retries: 20

  backend:
    build: ./backend
    shm_size: '4gb'
    container_name: hackaton-backend
    restart: unless-stopped
    command: python -m uvicorn main:application --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./backend:/app
    environment:
      - ML_PIPELINE_SERVICE=ml-pipeline
      - QUEUE_AMQP_URL=amqp://appuser:supersecret@rabbit:5672/%2F
    depends_on:
      rabbit:
        condition: service_healthy

  worker:
    build: ./backend
    restart: unless-stopped
    depends_on:
      rabbit:
        condition: service_healthy
    environment:
      - QUEUE_AMQP_URL=amqp://appuser:supersecret@rabbit:5672/%2F
      - ML_PIPELINE__URL=http://ml-pipeline:8080
      - BACKEND_BASE_URL=http://backend:8000
    command: >
      sh -c "
        until nc -z rabbit 5672; do echo 'waiting for rabbit...'; sleep 1; done &&
        python -c 'import src.tasks.ml_tasks as _' &&
        dramatiq src.tasks.ml_tasks --processes 1 --threads 4
      "

  scheduler:
    build:
      context: .
      dockerfile: backend/src/services/scheduler/Dockerfile
    restart: unless-stopped
    command: python -m services.scheduler.train
    volumes:
      - ./backend:/out
    environment:
      - QUEUE_AMQP_URL=amqp://appuser:supersecret@rabbit:5672/%2F
    depends_on:
      rabbit:
        condition: service_healthy

  ml-pipeline:
    build: ./ml
    container_name: hackaton-ml-pipeline
    shm_size: '4gb'
    restart: unless-stopped
    volumes:
      - ./backend:/out
      - ml-pipeline-storage:/app/local_storage
    environment:
      - OMP_NUM_THREADS=4
      - PYTHONUNBUFFERED=1
    # expose:
    #   - "8080"   # uncomment if you want to expose outside compose

  ml-training:
    build: ./ml-training
    container_name: hackaton-ml-training
    restart: unless-stopped
    volumes:
      - ./backend:/out

  postprocessing:
    build: ./postprocessing
    shm_size: '4gb'
    restart: unless-stopped
    volumes:
      - ./backend/var:/out/var
    environment:
      - LLM_MODEL=infidelis/GigaChat-20B-A3B-instruct-v1.5:q4_K_M
      - LLM_BASE_URL=http://gigachat:11434
    depends_on:
      - gigachat

  gigachat:
    build:
      context: ./postprocessing
      dockerfile: Dockerfile.llm
    container_name: gigachat
    restart: unless-stopped
    volumes:
      - ./gigachat:/root/.ollama
    environment:
      - OLLAMA_NUM_THREADS=64

  frontend:
    build: ./frontend
    container_name: frontend-app
    restart: unless-stopped
    ports:
      - "8354:80"
    environment:
      - NODE_ENV=production

volumes:
  ml-pipeline-storage:
    driver: local
